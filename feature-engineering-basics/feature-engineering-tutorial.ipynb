{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e685ada",
   "metadata": {
    "papermill": {
     "duration": 0.017261,
     "end_time": "2023-05-30T00:11:31.201793",
     "exception": false,
     "start_time": "2023-05-30T00:11:31.184532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Last Updated: 2023-05-29\n",
    "# Completed: 2023-05-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680ad7e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T00:11:31.233201Z",
     "iopub.status.busy": "2023-05-30T00:11:31.232812Z",
     "iopub.status.idle": "2023-05-30T00:11:32.586777Z",
     "shell.execute_reply": "2023-05-30T00:11:32.585604Z"
    },
    "papermill": {
     "duration": 1.372611,
     "end_time": "2023-05-30T00:11:32.589295",
     "exception": false,
     "start_time": "2023-05-30T00:11:31.216684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# 0. Importing Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca399b3",
   "metadata": {
    "papermill": {
     "duration": 0.014645,
     "end_time": "2023-05-30T00:11:32.618621",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.603976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Goal of Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847e996",
   "metadata": {
    "papermill": {
     "duration": 0.014242,
     "end_time": "2023-05-30T00:11:32.647650",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.633408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We might perform feature engineering to:\n",
    "- improve a model's predictive performance\n",
    "- reduce computational or data needs\n",
    "- improve interpretability of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd0a70",
   "metadata": {
    "papermill": {
     "duration": 0.014354,
     "end_time": "2023-05-30T00:11:32.676603",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.662249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A Guiding Principle of Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a062e",
   "metadata": {
    "papermill": {
     "duration": 0.014379,
     "end_time": "2023-05-30T00:11:32.705578",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.691199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- The key idea here is that a transformation you apply to a feature becomes in essence a part of the model itself.\n",
    "- We will try to linearise the parameter to the output that we are predicting.\n",
    "- Ex: A linear model fits poorly with only \"X\" as feature, if \"X\" and \"Y\" does not have a linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d259d5",
   "metadata": {
    "papermill": {
     "duration": 0.014241,
     "end_time": "2023-05-30T00:11:32.734820",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.720579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6515b42",
   "metadata": {
    "papermill": {
     "duration": 0.014238,
     "end_time": "2023-05-30T00:11:32.763659",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.749421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**A great first step is to construct a ranking with a feature utility metric, a function measuring associations between a feature and the target.** Then you can choose a smaller set of the most useful features to develop initially and have more confidence that your time will be well spent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b71d1",
   "metadata": {
    "papermill": {
     "duration": 0.014285,
     "end_time": "2023-05-30T00:11:32.792467",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.778182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The metric we'll use is called \"mutual information\". Mutual information is a lot like correlation in that it measures a relationship between two quantities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3fa13",
   "metadata": {
    "papermill": {
     "duration": 0.014273,
     "end_time": "2023-05-30T00:11:32.821397",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.807124",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Mutual information is a great general-purpose metric and especially useful at the start of feature development when you might not know what model you'd like to use yet. It is:\n",
    "\n",
    "- easy to use and interpret,\n",
    "- computationally efficient,\n",
    "- theoretically well-founded,\n",
    "- resistant to overfitting, and,\n",
    "- able to detect any kind of relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c3203",
   "metadata": {
    "papermill": {
     "duration": 0.014179,
     "end_time": "2023-05-30T00:11:32.850018",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.835839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Mutual Information and What it Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450aa92a",
   "metadata": {
    "papermill": {
     "duration": 0.014311,
     "end_time": "2023-05-30T00:11:32.878861",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.864550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Mutual information describes relationships in terms of uncertainty. **The mutual information (MI) between two quantities is a measure of the extent to which knowledge of one quantity reduces uncertainty about the other.** If you knew the value of a feature, how much more confident would you be about the target?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03cf6b",
   "metadata": {
    "papermill": {
     "duration": 0.014303,
     "end_time": "2023-05-30T00:11:32.907736",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.893433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Technical note: What we're calling uncertainty is measured using a quantity from information theory known as \"entropy\". The entropy of a variable means roughly: \"how many yes-or-no questions you would need to describe an occurance of that variable, on average.\" The more questions you have to ask, the more uncertain you must be about the variable. **Mutual information is how many questions you expect the feature to answer about the target.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e7082",
   "metadata": {
    "papermill": {
     "duration": 0.014367,
     "end_time": "2023-05-30T00:11:32.936719",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.922352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Interpreting Mutual Information Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997ce22",
   "metadata": {
    "papermill": {
     "duration": 0.014325,
     "end_time": "2023-05-30T00:11:32.965579",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.951254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**The least possible mutual information between quantities is 0.0.** When MI is zero, the quantities are independent: neither can tell you anything about the other. **Conversely, in theory there's no upper bound to what MI can be.** In practice though values above 2.0 or so are uncommon. (Mutual information is a logarithmic quantity, so it increases very slowly.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f0ba4",
   "metadata": {
    "papermill": {
     "duration": 0.014445,
     "end_time": "2023-05-30T00:11:32.994621",
     "exception": false,
     "start_time": "2023-05-30T00:11:32.980176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here are some things to remember when applying mutual information:\n",
    "\n",
    "- MI can help you to understand **the relative potential of a feature as a predictor of the target, considered by itself**.\n",
    "\n",
    "- It's possible for a feature to be very informative when interacting with other features, but not so informative all alone. **MI can't detect interactions between features.** It is a univariate metric.\n",
    "\n",
    "- The actual usefulness of a feature depends on the model you use it with. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn't mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a67b499",
   "metadata": {
    "papermill": {
     "duration": 0.01427,
     "end_time": "2023-05-30T00:11:33.023405",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.009135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Link to tutorial/exercise:\n",
    "\n",
    "- https://www.kaggle.com/code/ryanholbrook/mutual-information\n",
    "- https://www.kaggle.com/code/tsztungchau/exercise-mutual-information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2766c1",
   "metadata": {
    "papermill": {
     "duration": 0.014157,
     "end_time": "2023-05-30T00:11:33.051967",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.037810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d701159",
   "metadata": {
    "papermill": {
     "duration": 0.014211,
     "end_time": "2023-05-30T00:11:33.080570",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.066359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Mathematical Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b86472",
   "metadata": {
    "papermill": {
     "duration": 0.015098,
     "end_time": "2023-05-30T00:11:33.110179",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.095081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Research in the field. Apply mathematical transformations whenever there will be a useful relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d636f8",
   "metadata": {
    "papermill": {
     "duration": 0.014457,
     "end_time": "2023-05-30T00:11:33.139465",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.125008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Visualization tools will sometimes be helpful for us to understand the mathematical relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae10464",
   "metadata": {
    "papermill": {
     "duration": 0.014146,
     "end_time": "2023-05-30T00:11:33.168336",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.154190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea37c1",
   "metadata": {
    "papermill": {
     "duration": 0.014312,
     "end_time": "2023-05-30T00:11:33.197324",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.183012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Features describing **the presence or absence of something** often come in sets, \n",
    "the set of risk factors for a disease, say. You can aggregate such features by **creating a count**.\n",
    "\n",
    "**These features will be binary** (1 for Present, 0 for Absent) **or boolean** (True or False). \n",
    "In Python, booleans can be added up just as if they were integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa89f879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T00:11:33.228794Z",
     "iopub.status.busy": "2023-05-30T00:11:33.228008Z",
     "iopub.status.idle": "2023-05-30T00:11:33.271409Z",
     "shell.execute_reply": "2023-05-30T00:11:33.270307Z"
    },
    "papermill": {
     "duration": 0.061851,
     "end_time": "2023-05-30T00:11:33.273700",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.211849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "0  1  4\n",
       "1  2  5\n",
       "2  3  6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5\n",
      "1    7\n",
      "2    9\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe line row_sum = df.sum(axis=1) calculates the sum of values along each row of the DataFrame df. \\nThe axis=1 parameter indicates that the sum operation should be performed horizontally across each row.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "display(df)\n",
    "\n",
    "# 1. we sum up the values in each row\n",
    "# 2. then, apply the resulting value into axis = 1\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "row_sum = df.sum(axis=1)\n",
    "print(row_sum)  # Output: 0    5\n",
    "                #         1    7\n",
    "                #         2    9\n",
    "                #         dtype: int64\n",
    "\n",
    "\"\"\"\n",
    "The line row_sum = df.sum(axis=1) calculates the sum of values along each row of the DataFrame df. \n",
    "The axis=1 parameter indicates that the sum operation should be performed horizontally across each row.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8c471",
   "metadata": {
    "papermill": {
     "duration": 0.015058,
     "end_time": "2023-05-30T00:11:33.304501",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.289443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.3 Building-Up and Breaking-Down Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1618d0c4",
   "metadata": {
    "papermill": {
     "duration": 0.014827,
     "end_time": "2023-05-30T00:11:33.334434",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.319607",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When information is packed in form of strings, we can use the **\"str\" accessor** lects you apply string methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62d411",
   "metadata": {
    "papermill": {
     "duration": 0.015356,
     "end_time": "2023-05-30T00:11:33.365384",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.350028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Or we could also **join simple features (by python string operations)** into a composed feature if we had reason to believe there was some interaction in the combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a335230",
   "metadata": {
    "papermill": {
     "duration": 0.014924,
     "end_time": "2023-05-30T00:11:33.395761",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.380837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.4 Group Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4a7e6",
   "metadata": {
    "papermill": {
     "duration": 0.01475,
     "end_time": "2023-05-30T00:11:33.426098",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.411348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Group transforms**, which aggregate/combine information across multiple rows grouped by some category. \n",
    "\n",
    "With a group transform you can create features like: \"the average income of a person's state of residence,\" \n",
    "or \"the proportion of movies released on a weekday, by genre.\" \n",
    "\n",
    "If you had discovered a category interaction, a group transform over that categry could be something good to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42682f09",
   "metadata": {
    "papermill": {
     "duration": 0.015332,
     "end_time": "2023-05-30T00:11:33.457381",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.442049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The **mean** function is a built-in dataframe method, which means we can pass it as a string to transform. \n",
    "\n",
    "Other handy methods include **max, min, median, var, std, and count**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566b4e32",
   "metadata": {
    "papermill": {
     "duration": 0.014701,
     "end_time": "2023-05-30T00:11:33.488042",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.473341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Some other built-in functions could also come in handy, such as \n",
    "- .sample()\n",
    "- .drop()\n",
    "- .transform()\n",
    "- .drop_duplicates\n",
    "- .merge()\n",
    "- .groupby()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edf4fc",
   "metadata": {
    "papermill": {
     "duration": 0.014759,
     "end_time": "2023-05-30T00:11:33.517937",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.503178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Tips on Creating Features\n",
    "It's good to keep in mind your model's own strengths and weaknesses when creating features. Here are some guidelines:\n",
    "- Linear models learn sums and differences naturally, but can't learn anything more complex.\n",
    "\n",
    "- Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\n",
    "\n",
    "- Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\n",
    "\n",
    "- Tree models can learn to approximate almost any combination of features, but when a combination is especially important they can still benefit from having it explicitly created, especially when data is limited.\n",
    "\n",
    "- Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd6214",
   "metadata": {
    "papermill": {
     "duration": 0.014841,
     "end_time": "2023-05-30T00:11:33.547971",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.533130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Link to tutorial/exercise:\n",
    "\n",
    "- https://www.kaggle.com/code/ryanholbrook/creating-features\n",
    "- https://www.kaggle.com/code/tsztungchau/exercise-creating-features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9055cd",
   "metadata": {
    "papermill": {
     "duration": 0.014996,
     "end_time": "2023-05-30T00:11:33.579081",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.564085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Clustering With K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88360d81",
   "metadata": {
    "papermill": {
     "duration": 0.014848,
     "end_time": "2023-05-30T00:11:33.609512",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.594664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Clustering** simply means the assigning of data points to groups based upon how similar the points are to each other. A clustering algorithm makes \"birds of a feather flock together,\" so to speak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9decaffb",
   "metadata": {
    "papermill": {
     "duration": 0.01479,
     "end_time": "2023-05-30T00:11:33.639437",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.624647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 Cluster Labels as a Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e84a0",
   "metadata": {
    "papermill": {
     "duration": 0.014683,
     "end_time": "2023-05-30T00:11:33.669058",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.654375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It's important to remember that this Cluster feature is categorical.\n",
    "\n",
    "The motivating idea for adding cluster labels is that the clusters will break up complicated relationships across features into simpler chunks. \n",
    "\n",
    "Our model can then just learn the simpler chunks one-by-one instead having to learn the complicated whole all at once. \n",
    "\n",
    "It's a \"divide and conquer\" strategy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d49b05",
   "metadata": {
    "papermill": {
     "duration": 0.014783,
     "end_time": "2023-05-30T00:11:33.698916",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.684133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2 K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d6c36",
   "metadata": {
    "papermill": {
     "duration": 0.014797,
     "end_time": "2023-05-30T00:11:33.728822",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.714025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "K-means clustering is one of the great many clustering algorithms. It is meant to be intuitive and easy to apply in a feature engineering context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5635e7",
   "metadata": {
    "papermill": {
     "duration": 0.014788,
     "end_time": "2023-05-30T00:11:33.758656",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.743868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**K-means** clustering measures similarity using ordinary straight-line distance (Euclidean distance, in other words). \n",
    "\n",
    "It creates clusters by placing a number of points, called **centroids**, inside the feature-space. \n",
    "\n",
    "Each point in the dataset is assigned to the cluster of whichever centroid it's closest to. \n",
    "\n",
    "The \"k\" in \"k-means\" is how many centroids (that is, clusters) it creates. You define the k yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f8da6",
   "metadata": {
    "papermill": {
     "duration": 0.014781,
     "end_time": "2023-05-30T00:11:33.788560",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.773779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are three important parameters from scikit-learn's implementation: \"n_clusters\", \"max_iter\", and \"n_init\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120bed48",
   "metadata": {
    "papermill": {
     "duration": 0.014663,
     "end_time": "2023-05-30T00:11:33.818218",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.803555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It's a simple two-step process. The algorithm **starts by randomly initializing some predefined number (n_clusters) of centroids.** It then iterates over these two operations:\n",
    "\n",
    "1. assign points to the nearest cluster centroid\n",
    "2. move each centroid to minimize the distance to its points\n",
    "\n",
    "It iterates over these two steps until the centroids aren't moving anymore, or until some maximum number of iterations has passed **(max_iter)**.\n",
    "\n",
    "It often happens that the initial random position of the centroids ends in a poor clustering. For this reason the algorithm repeats a number of times **(n_init)** and returns the clustering that has the least total distance between each point and its centroid, the optimal clustering.\n",
    "\n",
    "You may need to increase the **max_iter** for a large number of clusters or **n_init** for a complex dataset. Ordinarily though the only parameter you'll need to choose yourself is **n_clusters** (k, that is). The best partitioning for a set of features depends on the model you're using and what you're trying to predict, so it's best to tune it like any hyperparameter (through cross-validation, say)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c54d57a",
   "metadata": {
    "papermill": {
     "duration": 0.014586,
     "end_time": "2023-05-30T00:11:33.847683",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.833097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Link to tutorial/exercise:\n",
    "\n",
    "- https://www.kaggle.com/code/ryanholbrook/clustering-with-k-means\n",
    "- https://www.kaggle.com/code/tsztungchau/exercise-clustering-with-k-means\n",
    "\n",
    "(reference)\n",
    "\n",
    "- https://www.kaggle.com/code/dansbecker/using-categorical-data-with-one-hot-encoding/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3897d68",
   "metadata": {
    "papermill": {
     "duration": 0.014615,
     "end_time": "2023-05-30T00:11:33.877432",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.862817",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b158b6",
   "metadata": {
    "papermill": {
     "duration": 0.014515,
     "end_time": "2023-05-30T00:11:33.906951",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.892436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76721502",
   "metadata": {
    "papermill": {
     "duration": 0.014873,
     "end_time": "2023-05-30T00:11:33.936679",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.921806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Just like clustering is **a partitioning of the dataset based on proximity**, you could think of PCA as **a partitioning of the variation in the data**. PCA is a great tool to help you discover important relationships in the data and can also be used to create more informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1ece2",
   "metadata": {
    "papermill": {
     "duration": 0.014895,
     "end_time": "2023-05-30T00:11:33.966769",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.951874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(Technical note: PCA is typically applied to **standardized data**. With standardized data \"variation\" means \"correlation\". With unstandardized data \"variation\" means \"covariance\". All data in this course will be standardized before applying PCA.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e267b",
   "metadata": {
    "papermill": {
     "duration": 0.014736,
     "end_time": "2023-05-30T00:11:33.996499",
     "exception": false,
     "start_time": "2023-05-30T00:11:33.981763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2 Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5c0d7",
   "metadata": {
    "papermill": {
     "duration": 0.014822,
     "end_time": "2023-05-30T00:11:34.026188",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.011366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The whole idea of PCA: **instead of describing the data with the original features, we describe it with its axes of variation. The axes of variation become the new features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e91d1",
   "metadata": {
    "papermill": {
     "duration": 0.014965,
     "end_time": "2023-05-30T00:11:34.056370",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.041405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The new features PCA constructs are actually just liear combinations (weighted sums) of the original features.\n",
    "\n",
    "These new features are called the **principal components** of the data. The weights themselves are called **loadings**. There will be as many principal components as there are features in the original dataset.\n",
    "\n",
    "A component's loadings tell us what variation it expresses through signs and magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075971a0",
   "metadata": {
    "papermill": {
     "duration": 0.014693,
     "end_time": "2023-05-30T00:11:34.086100",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.071407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PCA also tells us the amount of variation in each component. We can see from the figures that there is more variation in the data along the **Size** component than along the **Shape** component. PCA makes this precise through each component's **percent of explained variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b3c4d",
   "metadata": {
    "papermill": {
     "duration": 0.014904,
     "end_time": "2023-05-30T00:11:34.116078",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.101174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.3 PCA for Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035162f",
   "metadata": {
    "papermill": {
     "duration": 0.014887,
     "end_time": "2023-05-30T00:11:34.146030",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.131143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are two ways you could use PCA for feature engineering.\n",
    "\n",
    "1. The first way is to **use it as a descriptive technique**. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create -- a product of 'Height' and 'Diameter' if 'Size' is important, say, or a ratio of 'Height' and 'Diameter' if Shape is important. You could even try clustering on one or more of the high-scoring components.\n",
    "\n",
    "2. The second way is to **use the components themselves as features**. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:\n",
    "\n",
    "  - **Dimensionality reduction**: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\n",
    "  \n",
    "  - **Anomaly detection**: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\n",
    "  \n",
    "  - **Noise reduction**: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\n",
    "  \n",
    "  - **Decorrelation**: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\n",
    "  \n",
    "**PCA basically gives you direct access to the correlational structure of your data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01fb287",
   "metadata": {
    "papermill": {
     "duration": 0.014613,
     "end_time": "2023-05-30T00:11:34.175507",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.160894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Side notes:\n",
    "- PCA only works with numeric features, like continuous quantities or counts.\n",
    "- PCA is sensitive to scale. It's good practice to standardize your data before applying PCA, unless you know you have good reason not to.\n",
    "- Consider removing or constraining outliers, since they can have an undue influence on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6510f16",
   "metadata": {
    "papermill": {
     "duration": 0.014697,
     "end_time": "2023-05-30T00:11:34.204901",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.190204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Link to tutorial/exercise:\n",
    "\n",
    "- https://www.kaggle.com/code/ryanholbrook/principal-component-analysis\n",
    "- https://www.kaggle.com/code/tsztungchau/exercise-principal-component-analysis\n",
    "\n",
    "(reference)\n",
    "\n",
    "- N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba543e60",
   "metadata": {
    "papermill": {
     "duration": 0.014739,
     "end_time": "2023-05-30T00:11:34.234668",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.219929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20225c1",
   "metadata": {
    "papermill": {
     "duration": 0.014684,
     "end_time": "2023-05-30T00:11:34.264305",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.249621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c6e30",
   "metadata": {
    "papermill": {
     "duration": 0.014567,
     "end_time": "2023-05-30T00:11:34.293963",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.279396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The technique we'll look at in this lesson, **target encoding**, is instead meant *for categorical features*. It's a method of encoding categories as numbers, like one-hot or label encoding, with the difference that it also uses the target to create the encoding. This makes it what we call a **supervised** feature engineering technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a0462f",
   "metadata": {
    "papermill": {
     "duration": 0.014666,
     "end_time": "2023-05-30T00:11:34.323698",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.309032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.2 Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6833e5",
   "metadata": {
    "papermill": {
     "duration": 0.017973,
     "end_time": "2023-05-30T00:11:34.357015",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.339042",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A **target encoding** is any kind of encoding that replaces a feature's categories with some number derived from the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341cd18",
   "metadata": {
    "papermill": {
     "duration": 0.015002,
     "end_time": "2023-05-30T00:11:34.387055",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.372053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5.3 Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03a562",
   "metadata": {
    "papermill": {
     "duration": 0.014783,
     "end_time": "2023-05-30T00:11:34.416715",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.401932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Smoothing is usually used when:\n",
    "1. there are unknown categories that are filled in automatically by Pandas\n",
    "2. there are rare categories that may not represent the importance of the feature in the future, if more data are provided in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7ede1",
   "metadata": {
    "papermill": {
     "duration": 0.014822,
     "end_time": "2023-05-30T00:11:34.446582",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.431760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The idea is to **blend the in-category average with the overall average**. \n",
    "\n",
    "Rare categories get less weight on their category average, while missing categories just get the overall average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65974da6",
   "metadata": {
    "papermill": {
     "duration": 0.014862,
     "end_time": "2023-05-30T00:11:34.476582",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.461720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In pseudocode:\n",
    "\n",
    "**encoding = weight * in_category + (1 - weight) * overall**\n",
    "\n",
    "where weight is a value between 0 and 1 calculated from the category frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e469582",
   "metadata": {
    "papermill": {
     "duration": 0.014753,
     "end_time": "2023-05-30T00:11:34.506396",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.491643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "An easy way to determine the value for weight is to compute an m-estimate:\n",
    "\n",
    "**weight = n / (n + m)**\n",
    "\n",
    "where **\"n\"** is the total number of times that category occurs in the data. The parameter **\"m\"** determines the \"smoothing factor\". Larger values of m put more weight on the overall estimate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e6c90c",
   "metadata": {
    "papermill": {
     "duration": 0.015048,
     "end_time": "2023-05-30T00:11:34.536501",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.521453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "When choosing a value for \"m\", consider how noisy the categories to be:\n",
    "- the noiser, the larger value for \"m\", and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6716e4",
   "metadata": {
    "papermill": {
     "duration": 0.015279,
     "end_time": "2023-05-30T00:11:34.566870",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.551591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Use Cases for Target Encoding\n",
    "Target encoding is great for:\n",
    "\n",
    "- **High-cardinality features**: A feature with a large number of categories can be troublesome to encode: a one-hot encoding would generate too many features and alternatives, like a label encoding, might not be appropriate for that feature. A target encoding derives numbers for the categories using the feature's most important property: its relationship with the target.\n",
    "\n",
    "- **Domain-motivated features**: From prior experience, you might suspect that a categorical feature should be important even if it scored poorly with a feature metric. A target encoding can help reveal a feature's true informativeness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdef5cb",
   "metadata": {
    "papermill": {
     "duration": 0.014777,
     "end_time": "2023-05-30T00:11:34.596950",
     "exception": false,
     "start_time": "2023-05-30T00:11:34.582173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Link to tutorial/exercise:\n",
    "\n",
    "- https://www.kaggle.com/code/ryanholbrook/target-encoding\n",
    "- https://www.kaggle.com/code/tsztungchau/exercise-target-encoding\n",
    "\n",
    "(reference)\n",
    "\n",
    "- N/A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.082732,
   "end_time": "2023-05-30T00:11:35.534583",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-30T00:11:18.451851",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
